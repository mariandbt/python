{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55cea8a",
   "metadata": {},
   "source": [
    "<h1> ______________________________________ </h1>\n",
    "\n",
    "<h1> Full NEXT100 with fiber panels simulation mapping analysis </h1>\n",
    "<h1> ______________________________________ </h1>\n",
    "\n",
    "<p style=\"font-size: 17px; color: black;\"> In this Notebook we take as input the maps created in the previous notebook and use it to simulate an s2 signal for bb0nu events. </p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c28e7",
   "metadata": {},
   "source": [
    "<h1> ____________ </h1>\n",
    "<h2> This version </h2>\n",
    "<h1> ____________ </h1>\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> 100K ie S2 events. </p>\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Mapping separated by sensors. </p>\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Geant4 fundamental units: </p>\n",
    "\n",
    " -  <p style=\"font-size: 17px; color: black;\"> Length [L]: mm (milimeter) </p>\n",
    " -  <p style=\"font-size: 17px; color: black;\"> Time [T]: ns (nanosecond) </p>\n",
    " -  <p style=\"font-size: 17px; color: black;\"> Energy [E]: MeV (megaelectronvolt) </p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d793eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy              as np\n",
    "import pandas             as pd\n",
    "\n",
    "import scipy              \n",
    "from scipy                import stats, integrate\n",
    "from scipy.signal         import convolve\n",
    "\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "from matplotlib.ticker    import FormatStrFormatter # to set the format of the plot's axis\n",
    "from matplotlib.patches   import Rectangle # To add blanck spaces in tabular legends\n",
    "\n",
    "import os\n",
    "import math\n",
    "import tables             as tb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfa423",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting distributions to define\n",
    "\n",
    "# note: pdf are normilized to 1 so we have yo add an amplitude param\n",
    "def gauss(x, a,  mu, sigma):\n",
    "    return a*stats.norm.pdf(x, mu, sigma) \n",
    "\n",
    "def gauss_sum(x, a0,  mu0, sigma0, a1, mu1, sigma1):\n",
    "    return a0*stats.norm.pdf(x, mu0, sigma0)  + a1*stats.norm.pdf(x, mu1, sigma1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f19152",
   "metadata": {},
   "source": [
    "# Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/investigator/mariandbt/python/data'\n",
    "path = path + '/20231025_NEXT100_full_mapping'\n",
    "\n",
    "# filename = os.path.join(path, \"bb0nu_s2_signal.h5\")\n",
    "# filename = os.path.join(path, \"20231215_bb0nu_s2_signal.h5\")\n",
    "# filename = os.path.join(path, \"20231218_bb0nu_s2_signal.h5\")\n",
    "# filename = os.path.join(path, \"20231220_bb0nu_100ev_s2_signal.h5\")\n",
    "filename = os.path.join(path, \"20231226_bb0nu_100ev_s2_signal.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee61880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tb.open_file(filename) as file:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5913a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We read each file, make the histogram and save JUST the max value of that histogram in s2_max_dict\n",
    "\n",
    "import h5py\n",
    "\n",
    "# Load the 3D dictionary from the HDF5 file\n",
    "\n",
    "columns = {0:'time',\n",
    "           1:'s2'\n",
    "          }\n",
    "\n",
    "bin_width = 1000 # [ns] = 1 [us]\n",
    "s2_max_dict = {}\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File(filename, 'r') as file:\n",
    "    # Iterate through the top-level keys (groups) in the HDF5 file\n",
    "    for key in file.keys():\n",
    "        # Get the group corresponding to the current key\n",
    "        group = file[key]\n",
    "        \n",
    "        # Print the top-level key\n",
    "        print(f'Top-level key: {key}')\n",
    "        \n",
    "        # Iterate through the subkeys (datasets) in the current group\n",
    "        for subkey in group.keys():\n",
    "            \n",
    "            # Check if subkey is already in the dictionary\n",
    "            if subkey not in s2_max_dict:\n",
    "                s2_max_dict[subkey] = []\n",
    "            \n",
    "            # Get and print the value corresponding to the current subkey\n",
    "            signal = group[subkey][()]\n",
    "            signal = pd.DataFrame(signal)\n",
    "            signal.rename(columns = columns, inplace=True)\n",
    "            \n",
    "            t = signal.time\n",
    "            s2 = signal.s2\n",
    "            binin = np.arange(t.min() - bin_width, t.max() + 2*bin_width, bin_width)\n",
    "            \n",
    "            # Create a histogram\n",
    "            hist_values, bin_edges = np.histogram(t, bins=binin,\n",
    "                                                  weights = s2)\n",
    "            \n",
    "#             print(f'Subkey: {subkey}, Value: {value}')\n",
    "\n",
    "            s2_max_dict[subkey].append(hist_values.max())\n",
    "    \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aafbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s2_max_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf8bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s2_max_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = 'sens_221'\n",
    "\n",
    "s2 = s2_max_dict[sensor]\n",
    "s2 = np.array(s2)\n",
    "n_events = np.size(s2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize=(5,5), constrained_layout=True)\n",
    "\n",
    "bin_width = 1 # [e]\n",
    "n_bins = 100\n",
    "# binin = np.arange(s2.min() - bin_width, s2.max() + 2*bin_width, bin_width)\n",
    "binin = n_bins\n",
    "\n",
    "font_size = 8.5\n",
    "\n",
    "events, bins, bars = ax.hist(s2, binin, \n",
    "                             density=False,\n",
    "                             label='s2', \n",
    "                             histtype='step')\n",
    "\n",
    "ax.set_title(f'Max s2 signal in {sensor} for {n_events} events');\n",
    "ax.set_xlabel('s2 signal max [e]');\n",
    "ax.set_ylabel('Counts');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104bea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = np.array(list(s2_max_dict.values()))\n",
    "\n",
    "n_sensors = np.shape(s2)[0]\n",
    "n_events = np.shape(s2)[1]\n",
    "\n",
    "s2 = np.concatenate(s2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize=(5,5), constrained_layout=True)\n",
    "\n",
    "bin_width = 1 # [e]\n",
    "n_bins = 100\n",
    "# binin = np.arange(s2.min() - bin_width, s2.max() + 2*bin_width, bin_width)\n",
    "binin = n_bins\n",
    "\n",
    "font_size = 8.5\n",
    "\n",
    "events, bins, bars = ax.hist(s2, binin, \n",
    "                             density=False,\n",
    "                             label='s2: max value = %.2f'%(s2.max()), \n",
    "                             histtype='step')\n",
    "\n",
    "ax.set_title(f'Max s2 signal in all {n_sensors} sensors for {n_events} events');\n",
    "ax.set_xlabel('s2 signal max [e]');\n",
    "ax.set_ylabel('Counts');\n",
    "\n",
    "\n",
    "# _______________________________________fit_________________________________________________________________\n",
    "\n",
    "x_ax = np.linspace(bins.min(), bins.max(), len(bins)-1)\n",
    "\n",
    "# gaussian fit\n",
    "\n",
    "# popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, bounds=([0, 0, 1], [np.inf, 3e3, 1e2])) \n",
    "popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, p0=[1, bins.mean(), 1e2]) \n",
    "\n",
    "a, mu, sigma = popt\n",
    "best_fit_line = gauss(x_ax, a, mu, sigma)\n",
    "\n",
    "ax.plot(x_ax, best_fit_line, label = 'Gaussian fit: $\\mu$ = %.2f, $\\sigma$ = %.2f'%(mu, sigma))\n",
    "\n",
    "ax.legend(fontsize=10, loc='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a223c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
