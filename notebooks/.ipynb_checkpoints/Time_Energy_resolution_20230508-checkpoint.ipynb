{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55cea8a",
   "metadata": {},
   "source": [
    "<h1> ______________________________________ </h1>\n",
    "\n",
    "<h1> Time and Energy resolution calculation </h1>\n",
    "<h1> ______________________________________ </h1>\n",
    "\n",
    "<p style=\"font-size: 17px; color: black;\"> In this Notebook we calculate the energy and time resolution of our photosensor (SiPM Hamamatsu in this case) the following way:</p>\n",
    "\n",
    "- <p style=\"font-size: 17px; color: black; font-weight: bold;\"> Time resolution: </p> \n",
    "<p style=\"font-size: 17px; color: black;\"> We set a voltage threshold and we measure the time difference between when our signal and our trigger waveform pass that threshold. The std of that time difference distribution will be our time resolution. The time resolution is not the same for all signals so we differentiate among signals of 1pe, 2pe, 3pe, etc. </p> \n",
    "\n",
    "\n",
    "- <p style=\"font-size: 17px; color: black; font-weight: bold;\"> Energy resolution: </p> \n",
    "<p style=\"font-size: 17px; color: black;\"> We save for each waveform the value of the peak, then we do the distribution of that value for each waveform type, ie we do different distributions separating waveforms corresponding to 1pe, 2pe, 3pe, etc. The std of that peak distribution will be our energy resolution. </p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c28e7",
   "metadata": {},
   "source": [
    "<h1> ____________ </h1>\n",
    "<h2> This version </h2>\n",
    "<h1> ____________ </h1>\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> 10K events WITH Y11 FIBERS. </p>\n",
    "-  <p style=\"font-size: 17px; color: black;\"> The photosensor signal saturates (there are values considered inf). </p>\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Waveforms include just 1 peak. </p>\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Signal happens AFTER THE TRIGGER. </p>\n",
    "\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Use interpolation to get the EXACT time at which the waveform cuts the threshold. This way we can basically work with a function intead of the data. </p>\n",
    "\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Calculate chi2 with a built in function instead of manually. </p>\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Variable bining in the distributions as sqrt(Nevents). </p>\n",
    "\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Instead of obtaining the Npe peaks ad hoc we do it finding the peaks in the maximums distribution. </p>\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Same way use this distribution to determine dV from each peak to get rid of the in between events. </p>\n",
    "\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> We can choose between a variable or a fixed threshold with fix_th = True/False. </p>\n",
    "\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Energy distribution in % instead of absolute values. </p>\n",
    "-  <p style=\"font-size: 17px; color: black;\"> Fit the sigma vs $ N_{pe} $ plot to the corresponding cuadratic sum of terms. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd2ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae45a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151ece17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats, integrate\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import root, root_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89145d5e-f992-4295-bfdb-0bf6750f872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca48882",
   "metadata": {},
   "source": [
    "<h1> _________________________________________________________________________________________________________ </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a2dfe0",
   "metadata": {},
   "source": [
    "<h1> __________________________ </h1>\n",
    "<h2> Functions </h2>\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> <b> gauss(x, a,  mu, sigma): </b> Gaussian normalized to $a$. </p>\n",
    "\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> <b> f_cut(x, f, val): </b> To find the cut of a $f(x)$ function with a certain value $val$ we first use <b> f_cut </b> to move $f(x) \\rightarrow f(x) - val$ and then find it's roots, ie, $f(x) - val = 0$. </p>\n",
    "\n",
    "\n",
    "-  <p style=\"font-size: 17px; color: black;\"> <b> resolution_eq(E, a, b, c): </b> Resolution equation to which our results should fit. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c057ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian\n",
    "# note: pdf are normilized to 1 so we have yo add an amplitude param\n",
    "def gauss(x, a,  mu, sigma):\n",
    "# def gauss(x,  mu, sigma):\n",
    "    \n",
    "    gaussian = stats.norm.pdf(np.sort(x), mu, sigma) \n",
    "    \n",
    "    return (a/gaussian.sum())*gaussian \n",
    "#     return (x.sum()/gaussian.sum())*gaussian \n",
    "\n",
    "# Function offsetting\n",
    "# To find the cut of a function with a certain value first we offset the function then find the roots\n",
    "def f_cut(x, f, val):\n",
    "    return f(x) - val\n",
    "\n",
    "# Resolution equation\n",
    "def resolution_eq(E, a, b, c):\n",
    "    \n",
    "    res = np.sqrt( (a/np.sqrt(E))**2 + (b/E)**2 + c**2)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b38a7d",
   "metadata": {},
   "source": [
    "<h1> __________________________ </h1>\n",
    "<h2> Data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba914e0",
   "metadata": {},
   "source": [
    "-  <p style=\"font-size: 17px; color: black;\"> <b> Reading the file </b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbac72c",
   "metadata": {},
   "source": [
    "-  <p style=\"font-size: 17px; color: black;\"> File's path </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cee91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = []\n",
    "\n",
    "path = '/home/investigator/mariandbt/python/data/'\n",
    "path = path + '20230508/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803c911",
   "metadata": {},
   "source": [
    "-  <p style=\"font-size: 17px; color: black;\"> Read the file. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae7f0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using 3333 files out of 10000\n",
      " 0 files read so long \n",
      " 300 files read so long \n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "# Get all files in the folder\n",
    "files = os.listdir(path)\n",
    "max_files_used = int(len(files)/3)\n",
    "# max_files_used = int(300)\n",
    "# max_files_used = int(len(files))\n",
    "\n",
    "print('We are using %s files out of %s'%(max_files_used, len(files)))\n",
    "count = 0\n",
    "\n",
    "# Loop through each file \n",
    "for file_name in files[:max_files_used]:\n",
    "    file = os.path.join(path, file_name)\n",
    "    \n",
    "    if (count%300 == 0):\n",
    "        print(' %s files read so long '%(count))\n",
    "        \n",
    "    with open(file,'r', encoding='ascii') as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            if i > 15:\n",
    "                data.append(np.asarray(line.split(','), dtype=float))\n",
    "    count += 1\n",
    "                \n",
    "                \n",
    "fp.close()\n",
    "\n",
    "print('DONE READING!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8f4ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)\n",
    "max_files_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c1a01",
   "metadata": {},
   "source": [
    "-  <p style=\"font-size: 17px; color: black;\"> Make a copy of the data to work with, this way it's easier to re-start the copy instead of re-reading the file. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7facac73",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "waveform = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e45672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames = int(len(files))\n",
    "n_frames = int(max_files_used)\n",
    "n_points = int(len(waveform)/n_frames)\n",
    "\n",
    "print('n_points = ', n_points)\n",
    "print('n_frames = ', n_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5476af0e",
   "metadata": {},
   "source": [
    "<h1> __________________________ </h1>\n",
    "<h2> Global parameters </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5a7df",
   "metadata": {},
   "source": [
    "-  <p style=\"font-size: 17px; color: black;\"> <b> Analysis parameters </b> </p>\n",
    "\n",
    " -  <p style=\"font-size: 17px; color: black;\"> <b> time, LED, signal: </b> Respectively, the time, signal of the trigger, and signal of the photosensor. These are the avriables we'll use ALL along and are adjusted to the propper units. </p>\n",
    " \n",
    " -  <p style=\"font-size: 17px; color: black;\"> <b> Offsets: </b> Both the trigger (LED) and the photosensor signal have an offset with respect to 0, so we correct the both before working. </p>\n",
    " \n",
    " -  <p style=\"font-size: 17px; color: black;\"> <b> waveform_saturation: </b> The signal saturates at certain value. </p>\n",
    " \n",
    " -  <p style=\"font-size: 17px; color: black;\"> <b> dV: </b> When separating signals to differentiate between different photo-electron signals we define a validity region around the selected $N_{pe}$ value with width $dV$. </p>\n",
    " \n",
    " -  <p style=\"font-size: 17px; color: black;\"> <b> fix_th: </b> True if we want to use a fixed threshold (determined after the multi-pe peaks). If not, the threshold at which to measure the time difference between signal and trigger is variable depending on each waveform. </p>\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = waveform[:, 0]*1e6 # [us]\n",
    "LED = waveform[:, 2]*1e3 # [mV]\n",
    "signal = waveform[:, 1]*1e3 # [mV]\n",
    "\n",
    "# Offsets\n",
    "offset_LED = LED[LED < 0].mean(); LED = LED - offset_LED\n",
    "\n",
    "offset_signal = signal[signal < 0].mean(); signal = signal - offset_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afff527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satturation correction\n",
    "waveform_saturation = signal.max()\n",
    "signal = np.where(signal < waveform_saturation, signal, signal[signal < waveform_saturation].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dV = 25 # [mV] Voltage difference from the multi-pe peaks we're considering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd79b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_th = False # True if we want to use a fixed threshold (determined after the multi-pe peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394dca4",
   "metadata": {},
   "source": [
    "<h1> __________________________ </h1>\n",
    "<h2> Analysis </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770c3d4",
   "metadata": {},
   "source": [
    "<h2> Determination of the $N_{pe}$ peaks </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c461520",
   "metadata": {},
   "source": [
    "-  <p style=\"font-size: 17px; color: black;\"> Use a peak distribution and select this distribution peaks as the $N_{pe}$ values and $dV$ as the validity region around these value to consider a certain signal part of that $N_{pe}$ group. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87eff8",
   "metadata": {},
   "source": [
    "-  <p style=\"font-size: 17px; color: black;\"> Save the maximum of each waveform to do the distribution later. Since there are some waveforms that saturate we pick the second highest valid value. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_peaks = []\n",
    "\n",
    "for frame in range(n_frames):\n",
    "    \n",
    "    ss = signal[frame*n_points: (frame + 1)*n_points].copy() # [mV]\n",
    "    pe_peaks.append(ss.max())\n",
    "    \n",
    "pe_peaks = np.array(pe_peaks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91dd6a",
   "metadata": {},
   "source": [
    "-  <p style=\"font-size: 17px; color: black;\"> Set the parameters to later find the peaks in the distribution and determine the acceptance region around this peak. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e06dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_min = n_frames/250. # height threshold to which consider a peak\n",
    "height_min = 15. # height threshold to which consider a peak\n",
    "dist_min = 10. # min distance between peaks required\n",
    "dif_min = None # min height difference between the peak point and their neighbours "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378c94e",
   "metadata": {},
   "source": [
    "- <p style=\"font-size: 17px; color: black;\"> Waveform's maximums distribution and location of the photo-electrons peaks and the acceptance region around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,3), constrained_layout=True)\n",
    "events, bins, bars = ax.hist(pe_peaks, 200, \n",
    "                             density=False, \n",
    "                             label='Multi-photo-electron spectrum', \n",
    "                             histtype='step')\n",
    "x_ax = bins[:-1]\n",
    "diffs = np.abs(np.diff(bins))\n",
    "# accept_region = np.array(dV/diffs, dtype = int) # instead of defining the acceptance region by Voltage we define it in terms of bins\n",
    "accept_region = dV*np.ones(len(x_ax))\n",
    "\n",
    "peaks, _ = find_peaks(events, height=height_min, threshold=dif_min, distance=dist_min)\n",
    "\n",
    "for ii in range(len(peaks)):\n",
    "    ax.fill_between([(x_ax[peaks] - accept_region[peaks])[ii], \n",
    "                     (x_ax[peaks] + accept_region[peaks])[ii]], \n",
    "                    events[peaks].max(), \n",
    "                    color = 'magenta', alpha = 0.3, label = r'Peak regions ($\\pm$ %.2f mV)'%(dV)) \n",
    "\n",
    "ax.plot(x_ax[peaks], events[peaks], 'o', label = 'Identified peaks')\n",
    "ax.plot(x_ax, height_min*np.ones(len(x_ax)), label = 'Minimum height considered for a peak')\n",
    "\n",
    "ax.set_xlabel('Pe signal [mV]')\n",
    "ax.set_ylabel('Counts')\n",
    "\n",
    "hand, labl = ax.get_legend_handles_labels()\n",
    "hand, labl = np.array(hand), np.array(labl)\n",
    "not_duplicated = ~pd.DataFrame(labl).duplicated()\n",
    "\n",
    "ax.legend(hand[not_duplicated], labl[not_duplicated], loc = 'best')\n",
    "# ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36194e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_pe = x_ax[peaks]\n",
    "n_pe = len(signal_pe)\n",
    "print('Npe = ', n_pe)\n",
    "print('Npe values = ', signal_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8065cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import copy\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "my_cmap = copy.copy(plt.get_cmap('plasma')) # copy the default cmap\n",
    "my_cmap.set_bad(my_cmap.colors[0]) #  set the color of invalid or masked values in the colormap\n",
    "\n",
    "\n",
    "counts, xedges, yedges, im = ax.hist2d(time[:500*n_points], signal[:500*n_points], \n",
    "                                       bins=100, rasterized=True, \n",
    "                                       range=((time.min(),time.max()), (signal.min(), signal.max())), \n",
    "                                       cmap=my_cmap, norm=mpl.colors.LogNorm(vmin = 1))\n",
    "\n",
    "for ii in range(n_pe):\n",
    "    ax.plot(0.53, signal_pe[ii], 'ow', markersize = 3, label = '%s pe'%(ii))\n",
    "    \n",
    "\n",
    "\n",
    "ax.set_xlabel(r\"Time [$\\mathrm{\\mu}$s]\", size='larger')\n",
    "ax.set_ylabel(\"Voltage [mV]\", size='larger')\n",
    "\n",
    "# ax.legend(loc = 'best')\n",
    "\n",
    "fig.colorbar(im, ax = ax, orientation='vertical')\n",
    "# fig.savefig(\"waveforms.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddec76d",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Set fixed threshold to 50% of the single photo-electron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ef221",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_fixed = .5*signal_pe[0] # [mV] Height at which we measure the time difference between LED and Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430ba52",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black; font-weight: bold\"> Waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5fb335",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Interpolate the waveform to create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 23\n",
    "frame = 789\n",
    "frame = 3151\n",
    "# frame = 1755; th = 57.38 # [mV]\n",
    "# frame = 2031; th = 131.43 # [mV]\n",
    "# frame = 266; th = 114.13 # [mV]\n",
    "# frame = 1712\n",
    "frame = 2834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = time[frame*n_points: (frame + 1)*n_points].copy()  # [us]\n",
    "ss = signal[frame*n_points: (frame + 1)*n_points].copy() # [mV]\n",
    "ll = LED[frame*n_points: (frame + 1)*n_points].copy() # [mV]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82832f38",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Time region where to look for signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fdb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_min = 0.45 # [us]\n",
    "tt_max = 0.7 # [us]\n",
    "\n",
    "tt_region_mask = (tt > tt_min) & (tt < tt_max) # Where to look for the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d5226",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Threshold to consider a peak in this waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "th = max(0.8*signal_pe[0], min(0.8*ss[tt_region_mask].max(), 0.8*ll.max()))\n",
    "\n",
    "if fix_th:\n",
    "    th = th_fixed\n",
    "            \n",
    "print('threshold = %.2f mV'%(th))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad435e",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Data waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d53768",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (15, 5), constrained_layout=True)\n",
    "\n",
    "ax.plot(tt, ll, label = 'LED')\n",
    "ax.plot(tt, ss, label = 'signal')\n",
    "ax.plot(tt, th*np.ones(len(tt)), label = 'threshold')\n",
    "\n",
    "ax.fill_between([tt_min, tt_max], max(ll.max(), ss.max()), \n",
    "                color = 'peachpuff', alpha = 0.5, label = r'Time region for signal') \n",
    "\n",
    "ax.set_ylabel('Voltage [mV]');\n",
    "ax.set_xlabel('Time [us]');\n",
    "ax.set_title('Data waveform', size = 20);\n",
    "ax.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbe93c",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Waveform interpolation and finding the cut with the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interpolation functions\n",
    "f_ss = interp1d(tt, ss)\n",
    "f_ll = interp1d(tt, ll)\n",
    "\n",
    "# Define the intervals where to search for roots\n",
    "t0 = tt[ss - th < 0.][0]\n",
    "tf_ss = tt[(tt > tt_min) & (ss - th > 0.)][0]\n",
    "tf_ll = tt[ll - th > 0.][0]\n",
    "\n",
    "# Find the roots of the new function for val = th\n",
    "root_ss = root_scalar(f_cut, args=(f_ss, th,), bracket=[t0, tf_ss]).root\n",
    "root_ll = root_scalar(f_cut, args=(f_ll, th,), bracket=[tt.min(), tf_ll]).root\n",
    "\n",
    "dt = np.fabs(root_ss - root_ll)\n",
    "print('dt = %.2f us'%(dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429299e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (15, 5), constrained_layout=True)\n",
    "\n",
    "ax.plot(tt, f_ll(tt), label = 'LED interpolation')\n",
    "ax.plot(tt, f_ss(tt), label = 'Signal interpolation')\n",
    "ax.plot(tt, th*np.ones(len(tt)), label = 'threshold')\n",
    "\n",
    "ax.fill_between([tt_min, tt_max], max(ll.max(), ss.max()), \n",
    "                color = 'peachpuff', alpha = 0.5, label = r'Time region for signal') \n",
    "\n",
    "ax.plot(root_ss, th, 'o', markersize = 5, label = 'Signal cut with threshold')\n",
    "ax.plot(root_ll, th, 'o', markersize = 5, label = 'LED cut with threshold')\n",
    "ax.plot(np.linspace(root_ss, root_ll, len(tt)), th*np.ones(len(tt)), '-m', label = 'dt = %.2f us'%(dt))\n",
    "\n",
    "ax.set_ylabel('Voltage [mV]');\n",
    "ax.set_xlabel('Time [us]');\n",
    "ax.set_title('Interpolated waveform', size = 20);\n",
    "ax.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d930dc0",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Difference in time calculation for all frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740743ae",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 15px; color: black;\"> Note how we're working with the fact that the signal appears in a certain time region to constrain the valid region of signal we use using the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50b936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt = []\n",
    "s_peak = []\n",
    "\n",
    "try:\n",
    "    for frame in range(n_frames):\n",
    "\n",
    "        tt = time[frame*n_points: (frame + 1)*n_points].copy()  # [us]\n",
    "        ss = signal[frame*n_points: (frame + 1)*n_points].copy() # [mV]\n",
    "        ll = LED[frame*n_points: (frame + 1)*n_points].copy() # [mV]\n",
    "        \n",
    "        tt_region_mask = (tt > tt_min) & (tt < tt_max) # Where to look for the signal\n",
    "\n",
    "        th = max(0.8*signal_pe[0], min(0.8*ss[tt_region_mask].max(), 0.8*ll.max()))\n",
    "        \n",
    "        if fix_th:\n",
    "            th = th_fixed\n",
    "        \n",
    "        if ss[tt_region_mask].max() < th: \n",
    "            print('NO SIGNAL HERE! frame = %s; th = %.2f # [mV]'%(frame, th))\n",
    "            continue\n",
    "            \n",
    "        if ss[tt_region_mask][0] > th: \n",
    "            print('This is not signal, it is the tail of a noise pulse! frame = %s; th = %.2f # [mV]'%(frame, th))\n",
    "            continue\n",
    "\n",
    "        # Define the interpolation functions\n",
    "        f_ss = interp1d(tt, ss) \n",
    "        f_ll = interp1d(tt, ll)\n",
    "        \n",
    "        # Define the intervals where to search for roots\n",
    "        t0 = tt[ss - th < 0.][0]\n",
    "        tf_ss = tt[(tt > tt_min) & (ss - th > 0.)][0]\n",
    "        tf_ll = tt[ll - th > 0.][0]\n",
    "\n",
    "        # Find the roots of the new function for val = th\n",
    "        root_ss = root_scalar(f_cut, args=(f_ss, th,), bracket=[t0, tf_ss]).root\n",
    "        root_ll = root_scalar(f_cut, args=(f_ll, th,), bracket=[tt.min(), tf_ll]).root\n",
    "\n",
    "        s_peak.append(ss.max())\n",
    "        dt.append(np.fabs(root_ss - root_ll))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Python sais:\", str(e))\n",
    "    print(\"Marian sais: Error accured in frame \", frame)\n",
    "    \n",
    "dt = np.array(dt)\n",
    "s_peak = np.array(s_peak)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be87a3",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Peak identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pe = []\n",
    "for i in range(n_pe):\n",
    "#     mask_pe.append(np.fabs(s_peak - signal_pe[i]) <= dV) # for fixed dV\n",
    "    mask_pe.append(np.fabs(s_peak - signal_pe[i]) <= accept_region[i]) # for fixed number of bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873324a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (15, 10), constrained_layout=True)\n",
    "\n",
    "#______________________plot settings________________________________________________________\n",
    "\n",
    "x_ax = np.arange(len(s_peak))\n",
    "\n",
    "ax.plot(x_ax, s_peak, 'o', label = 'All peaks')\n",
    "\n",
    "for ii in range(len(mask_pe)):\n",
    "    ax.plot(x_ax[mask_pe[ii]], s_peak[mask_pe[ii]], 'o', label = '%s pe'%(ii + 1))\n",
    "\n",
    "ax.set_ylabel('Waveform peak [mV]');\n",
    "ax.set_xlabel('N_event');\n",
    "\n",
    "# ax.set_xlim([0, 20])\n",
    "# ax.set_ylim([s_peak.min(), 150])\n",
    "ax.legend(loc = 'best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90458592",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Time resolution calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f55c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "observed = np.array([10, 20, 30])\n",
    "expected = np.array([15, 15, 30])\n",
    "\n",
    "chi2, p_value = chisquare(observed, expected)\n",
    "\n",
    "print('chi^2 =', chi2)\n",
    "print('p-value =', p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(events), len(bins[:-1]), len(x_ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6714d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = math.ceil(n_pe/2.), ncols = 2, figsize=(10,10), constrained_layout=True)\n",
    "\n",
    "n_row = 0\n",
    "\n",
    "t_res = np.zeros(n_pe)\n",
    "t_res_err = np.zeros(n_pe)\n",
    "\n",
    "for pe in range(n_pe):\n",
    "# for pe in range(8, 9):\n",
    "    \n",
    "    n_col = pe%2\n",
    "    \n",
    "    N_ev = len(dt[mask_pe[pe]])\n",
    "#     n_bins = int(np.sqrt(N_ev))\n",
    "    n_bins = 10*int(np.sqrt(N_ev))\n",
    "#     n_bins = 100\n",
    "    \n",
    "    region_min = 0.022\n",
    "    region_max = .06\n",
    "    if (pe + 1 >= 6):\n",
    "        region_max = 0.045\n",
    "    if (pe + 1 > 7):\n",
    "        region_max = 0.04\n",
    "    \n",
    "    events, bins, bars = ax[n_row, n_col].hist(dt[mask_pe[pe]], n_bins, \n",
    "                                               density=False,\n",
    "#                                                range=(region_min, region_max),\n",
    "                                               label='dt distribution', \n",
    "                                               histtype='step')\n",
    "    \n",
    "    x_ax = np.linspace(bins.min(), bins.max(), len(bins)-1)\n",
    "    y_err = np.sqrt(events)\n",
    "    \n",
    "    ev_th = 0\n",
    "    if pe == 9 - 1:\n",
    "        ev_th = 2\n",
    "    else:\n",
    "        ev_th = 5\n",
    "    \n",
    "    \n",
    "    # gaussian fit\n",
    "    popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, \n",
    "                                          sigma = np.where(y_err > 0., y_err, 1e-3),\n",
    "                                          p0=[events.sum(), (bins[:-1])[events > ev_th].mean(), 0.001])\n",
    "#     popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, \n",
    "#                                           bounds=([1,  0.9*bins.mean(), 1e-4], [3., bins.mean(), 1e-2])) \n",
    "\n",
    "\n",
    "#     best_fit_line = gauss(x_ax, *popt)\n",
    "    best_fit_line = gauss(x_ax, *popt)*events.sum()/gauss(x_ax, *popt).sum()\n",
    "    a, mu, sigma = popt\n",
    "    \n",
    "    param_err = np.sqrt(np.diag(pcov))\n",
    "    a_err, mu_err, sigma_err = param_err\n",
    "    \n",
    "    # Calculate p-value and chi-square\n",
    "    fit_norm = gauss(x_ax, *popt)*events.sum()/gauss(x_ax, *popt).sum() # need to normalize the fit \n",
    "    chi2, p_value = stats.chisquare(f_obs = events, f_exp = fit_norm, axis = None)\n",
    "    chi2 = chi2/events.sum()\n",
    "#     chi2, p_value = stats.chisquare(f_obs = events, f_exp = gauss(x_ax, *popt))    \n",
    "    \n",
    "#     residuals = events - gauss(x_ax, *popt)\n",
    "#     data_err = np.sqrt(events)\n",
    "    \n",
    "#     chi2 = np.sum( np.where(data_err > 0., (residuals/ data_err)**2, 0.) );\n",
    "    \n",
    "#     print(chi2, stats.)\n",
    "    \n",
    "    t_res[pe] = sigma\n",
    "    t_res_err[pe] = sigma_err\n",
    "\n",
    "    #_________plot_settings_________________________________________________\n",
    "\n",
    "    ax[n_row, n_col].plot(x_ax, gauss(x_ax, a, mu, sigma), '-', \n",
    "                          label='$\\mu$ = %.2f, $\\sigma$ = %.2E, $\\chi^{2}$ = %.2E'%(mu, sigma, chi2))\n",
    "    ax[n_row, n_col].plot(x_ax, fit_norm, '-', \n",
    "                          label='fit_norm')\n",
    "    \n",
    "    ax[n_row, n_col].text(0.05, .9, str(pe+1) + ' pe', transform=ax[n_row, n_col].transAxes,\n",
    "                fontsize=10, verticalalignment='top', fontfamily='serif',\n",
    "                bbox=dict(facecolor='1.', edgecolor='none', pad=3.0))\n",
    "\n",
    "    ax[n_row, n_col].set_ylabel('Counts')\n",
    "    ax[n_row, n_col].set_xlabel('dt [us]')\n",
    "\n",
    "    ax[n_row, n_col].legend(fontsize=7, loc='upper right')\n",
    "    \n",
    "    n_row += pe%2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b0a0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = math.ceil(n_pe/2.), ncols = 2, figsize=(10,10), constrained_layout=True)\n",
    "\n",
    "n_row = 0\n",
    "\n",
    "t_res = np.zeros(n_pe)\n",
    "t_res_err = np.zeros(n_pe)\n",
    "\n",
    "for pe in range(n_pe):\n",
    "# for pe in range(8, 9):\n",
    "    \n",
    "    n_col = pe%2\n",
    "    \n",
    "    N_ev = len(dt[mask_pe[pe]])\n",
    "    n_bins = int(np.sqrt(N_ev))\n",
    "    n_bins = 10*int(np.sqrt(N_ev))\n",
    "#     n_bins = 100\n",
    "    \n",
    "    region_min = 0.022\n",
    "    region_max = .06\n",
    "    if (pe + 1 >= 6):\n",
    "        region_max = 0.045\n",
    "    if (pe + 1 > 7):\n",
    "        region_max = 0.04\n",
    "    \n",
    "    events, bins, bars = ax[n_row, n_col].hist(dt[mask_pe[pe]], n_bins, \n",
    "                                               density=False,\n",
    "#                                                range=(region_min, region_max),\n",
    "                                               label='dt distribution', \n",
    "                                               histtype='step')\n",
    "    \n",
    "    x_ax = np.linspace(bins.min(), bins.max(), len(bins)-1)\n",
    "    y_err = np.sqrt(events)\n",
    "    \n",
    "    ev_th = 0\n",
    "    if pe == 9 - 1:\n",
    "        ev_th = 2\n",
    "    else:\n",
    "        ev_th = 5\n",
    "    \n",
    "    \n",
    "    # gaussian fit\n",
    "    popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, \n",
    "                                          sigma = np.where(y_err > 0., y_err, 1e-3),\n",
    "                                          p0=[events.sum(), (bins[:-1])[events > ev_th].mean(), 0.001])\n",
    "#     popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, \n",
    "#                                           bounds=([1,  0.9*bins.mean(), 1e-4], [3., bins.mean(), 1e-2])) \n",
    "\n",
    "\n",
    "#     best_fit_line = gauss(x_ax, *popt)\n",
    "    best_fit_line = gauss(x_ax, *popt)*events.sum()/gauss(x_ax, *popt).sum()\n",
    "    a, mu, sigma = popt\n",
    "    \n",
    "    param_err = np.sqrt(np.diag(pcov))\n",
    "    a_err, mu_err, sigma_err = param_err\n",
    "    \n",
    "    # Calculate p-value and chi-square\n",
    "#     fit_norm = gauss(x_ax, *popt)*events.sum()/gauss(x_ax, *popt).sum() # need to normalize the fit \n",
    "#     chi2, p_value = stats.chisquare(f_obs = events, f_exp = fit_norm, axis = None)\n",
    "#     chi2 = chi2/events.sum()\n",
    "#     chi2, p_value = stats.chisquare(f_obs = events, f_exp = gauss(x_ax, *popt))    \n",
    "    \n",
    "    residuals = events - gauss(x_ax, *popt)\n",
    "    data_err = np.sqrt(events)\n",
    "    \n",
    "    chi2 = np.sum( np.where(data_err > 0., (residuals/ data_err)**2, 0.) );\n",
    "    chi2 /= len(bins)\n",
    "    \n",
    "    t_res[pe] = sigma\n",
    "    t_res_err[pe] = sigma_err\n",
    "\n",
    "    #_________plot_settings_________________________________________________\n",
    "\n",
    "    ax[n_row, n_col].plot(x_ax, gauss(x_ax, a, mu, sigma), '-', \n",
    "                          label='$\\mu$ = %.2f, $\\sigma$ = %.2E, $\\chi^{2}$ = %.2E'%(mu, sigma, chi2))\n",
    "    \n",
    "    ax[n_row, n_col].text(0.05, .9, str(pe+1) + ' pe', transform=ax[n_row, n_col].transAxes,\n",
    "                fontsize=10, verticalalignment='top', fontfamily='serif',\n",
    "                bbox=dict(facecolor='1.', edgecolor='none', pad=3.0))\n",
    "\n",
    "    ax[n_row, n_col].set_ylabel('Counts')\n",
    "    ax[n_row, n_col].set_xlabel('dt [us]')\n",
    "\n",
    "    ax[n_row, n_col].legend(fontsize=7, loc='upper right')\n",
    "    \n",
    "    n_row += pe%2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef2563",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = math.ceil(n_pe/2.), ncols = 2, figsize=(10,10), constrained_layout=True)\n",
    "\n",
    "n_row = 0\n",
    "\n",
    "t_res = np.zeros(n_pe)\n",
    "t_res_err = np.zeros(n_pe)\n",
    "\n",
    "for pe in range(n_pe):\n",
    "# for pe in range(8, 9):\n",
    "    \n",
    "    n_col = pe%2\n",
    "    \n",
    "    N_ev = len(dt[mask_pe[pe]])\n",
    "    n_bins = int(np.sqrt(N_ev))\n",
    "    n_bins = 10*int(np.sqrt(N_ev))\n",
    "#     n_bins = 100\n",
    "    \n",
    "    region_min = 0.5\n",
    "    region_max = .58\n",
    "#     if (pe + 1 >= 6):\n",
    "#         region_max = 0.045\n",
    "#     if (pe + 1 > 7):\n",
    "#         region_max = 0.04\n",
    "    \n",
    "    events, bins, bars = ax[n_row, n_col].hist(dt[mask_pe[pe]], n_bins, \n",
    "                                               density=False,\n",
    "                                               range=(region_min, region_max),\n",
    "                                               label='dt distribution', \n",
    "                                               histtype='step')\n",
    "    \n",
    "    x_ax = np.linspace(bins.min(), bins.max(), len(bins)-1)\n",
    "    y_err = np.where(events > 0., np.sqrt(events), 1.)\n",
    "    \n",
    "    ev_th = 0\n",
    "    if pe == 9 - 1:\n",
    "        ev_th = 2\n",
    "    else:\n",
    "        ev_th = 5\n",
    "    \n",
    "    # gaussian fit\n",
    "    popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, \n",
    "                                          sigma = np.where(y_err > 0., y_err, 1e-3),\n",
    "                                          p0=[events.sum(), (bins[:-1])[events > ev_th].mean(), 0.001])\n",
    "#                                           bounds=([1,  0.9*bins.mean(), 1e-4], [3., bins.mean(), 1e-2])) \n",
    "\n",
    "\n",
    "#     best_fit_line = gauss(x_ax, *popt)\n",
    "    best_fit_line = gauss(x_ax, *popt)*events.sum()/gauss(x_ax, *popt).sum()\n",
    "    a, mu, sigma = popt\n",
    "    \n",
    "#     print(events.sum(), best_fit_line.sum())\n",
    "    \n",
    "    param_err = np.sqrt(np.diag(pcov))\n",
    "    a_err, mu_err, sigma_err = param_err\n",
    "    \n",
    "    # Calculate p-value and chi-square\n",
    "    fit_norm = gauss(x_ax, *popt)*events.sum()/gauss(x_ax, *popt).sum() # need to normalize the fit \n",
    "    chi2, p_value = stats.chisquare(f_obs = events, f_exp = fit_norm, axis = None)\n",
    "#     print(chi2)\n",
    "#     chi2 = chi2/events.sum()\n",
    "#     print(chi2)\n",
    "#     chi2, p_value = stats.chisquare(f_obs = events, f_exp = gauss(x_ax, *popt))    \n",
    "    \n",
    "#     residuals = events - gauss(x_ax, *popt)\n",
    "#     data_err = np.sqrt(events)\n",
    "    \n",
    "#     chi2 = np.sum( np.where(data_err > 0., (residuals/ data_err)**2, 0.) );\n",
    "    \n",
    "#     print(chi2, stats.)\n",
    "    \n",
    "    t_res[pe] = sigma\n",
    "    t_res_err[pe] = sigma_err\n",
    "\n",
    "    #_________plot_settings_________________________________________________\n",
    "\n",
    "    ax[n_row, n_col].plot(x_ax, gauss(x_ax, a, mu, sigma), '-', \n",
    "                          label='$\\mu$ = %.2f, $\\sigma$ = %.2E, $\\chi^{2}$ = %.2E'%(mu, sigma, chi2))\n",
    "    \n",
    "    ax[n_row, n_col].text(0.05, .9, str(pe+1) + ' pe', transform=ax[n_row, n_col].transAxes,\n",
    "                fontsize=10, verticalalignment='top', fontfamily='serif',\n",
    "                bbox=dict(facecolor='1.', edgecolor='none', pad=3.0))\n",
    "\n",
    "    ax[n_row, n_col].set_ylabel('Counts')\n",
    "    ax[n_row, n_col].set_xlabel('dt [us]')\n",
    "\n",
    "    ax[n_row, n_col].legend(fontsize=7, loc='upper right')\n",
    "    \n",
    "    n_row += pe%2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), constrained_layout=True)\n",
    "\n",
    "x_ax = np.arange(len(t_res)) + 1\n",
    "\n",
    "ax.errorbar(x_ax, t_res*1e3, yerr = t_res_err*1e3, fmt = '-o', label = 'Time resolution vs number of photoelectrons')\n",
    "\n",
    "ax.set_ylabel('Time resolution [ns]')\n",
    "ax.set_xlabel('Number of photoelectrons')\n",
    "\n",
    "\n",
    "# Resolution fit\n",
    "#     popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, p0=[1, bins.mean(), 20.]) \n",
    "popt, pcov = scipy.optimize.curve_fit(resolution_eq, x_ax, t_res*1e3, \n",
    "                                      bounds=([0., 0., 0.], [np.inf, np.inf, np.inf])) \n",
    "\n",
    "aa, bb, cc = popt\n",
    "\n",
    "\n",
    "# Calculate the residuals and chi-square\n",
    "residuals = t_res*1e3 - resolution_eq(x_ax, *popt)\n",
    "data_err = t_res_err*1e3\n",
    "\n",
    "chi2 = np.sum(np.where(data_err > 0., (residuals/ data_err)**2, 0.))\n",
    "\n",
    "\n",
    "x_ax_fit = np.linspace(x_ax.min(), x_ax.max(), 100)\n",
    "best_fit_line = resolution_eq(x_ax_fit, aa, bb, cc)\n",
    "\n",
    "ax.plot(x_ax_fit, best_fit_line, '-', \n",
    "        label='Theoretical fit: a = %.2E, b = %.2f, c = %.2f; $\\chi^{2} = %.2f$'%(aa, bb, cc, chi2))\n",
    "\n",
    "ax.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a18a6c",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 17px; color: black;\"> Energy resolution calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=int(np.ceil(n_pe/2)), ncols=2, figsize=(10,10), constrained_layout=True)\n",
    "\n",
    "n_row = 0\n",
    "\n",
    "e_res = np.zeros(n_pe)\n",
    "e_res_err = np.zeros(n_pe)\n",
    "\n",
    "for pe in range(n_pe):\n",
    "    \n",
    "    n_col = pe%2\n",
    "    \n",
    "    N_ev = len(s_peak[mask_pe[pe]])\n",
    "#     n_bins = 5*int(np.sqrt(N_ev))\n",
    "    n_bins = int(np.sqrt(N_ev))\n",
    "    \n",
    "    events, bins, bars = ax[n_row, n_col].hist(s_peak[mask_pe[pe]], n_bins, \n",
    "                                     density=True, \n",
    "#                                                range=(0.08, 0.15),\n",
    "                                     label='Peak distribution', \n",
    "                                     histtype='step')\n",
    "    x_ax = np.linspace(bins.min(), bins.max(), len(bins)-1)\n",
    "\n",
    "#     # gaussian fit\n",
    "#     popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, p0=[1, bins.mean(), 20.]) \n",
    "    popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, \n",
    "                                          sigma = np.where(events > 0., np.sqrt(events), 1e-3),\n",
    "                                          bounds=([events.sum(),  0.9*bins.mean(), 10.], [np.inf, bins.mean(), 60.])) \n",
    "    \n",
    "    best_fit_line = gauss(x_ax, *popt)\n",
    "    a, mu, sigma = popt\n",
    "    \n",
    "    print(events.sum(), best_fit_line.sum(), a)\n",
    "    \n",
    "    param_err = np.sqrt(np.diag(pcov))\n",
    "    a_err, mu_err, sigma_err = param_err\n",
    "    \n",
    "    # Calculate p-value and chi-square\n",
    "    fit_norm = best_fit_line*events.sum()/best_fit_line.sum() # need to normalize the fit \n",
    "    chi2, p_value = stats.chisquare(f_obs = events, f_exp = fit_norm, axis = None)\n",
    "#     chi2, p_value = stats.chisquare(f_obs = events, f_exp = best_fit_line, axis = None)\n",
    "    \n",
    "    e_res[pe] = 100*sigma/mu\n",
    "    e_res_err[pe] = 100*np.sqrt( (sigma_err/mu)**2 + (sigma*mu_err/mu**2)**2 )\n",
    "\n",
    "    #_________plot_settings_________________________________________________\n",
    "\n",
    "    ax[n_row, n_col].plot(x_ax, best_fit_line, '-', \n",
    "                          label='$\\mu$ = %.2f, $\\sigma$ = %.2E, $\\chi^{2}$ = %.2f'%(mu, sigma, chi2))\n",
    "    \n",
    "    ax[n_row, n_col].text(0.05, .9, str(pe+1) + ' pe', transform=ax[n_row, n_col].transAxes,\n",
    "                fontsize=10, verticalalignment='top', fontfamily='serif',\n",
    "                bbox=dict(facecolor='1.', edgecolor='none', pad=3.0))\n",
    "\n",
    "    ax[n_row, n_col].set_ylabel('Counts')\n",
    "    ax[n_row, n_col].set_xlabel('Peak Voltage [mV]')\n",
    "\n",
    "    ax[n_row, n_col].legend(fontsize=7, loc='best')\n",
    "    \n",
    "    n_row += pe%2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), constrained_layout=True)\n",
    "\n",
    "x_ax = np.arange(len(e_res)) + 1\n",
    "\n",
    "\n",
    "ax.errorbar(x_ax, e_res, yerr = e_res_err, fmt = '-o', label = 'Energy resolution vs number of photoelectrons')\n",
    "\n",
    "# Resolution fit\n",
    "#     popt, pcov = scipy.optimize.curve_fit(gauss, x_ax, events, p0=[1, bins.mean(), 20.]) \n",
    "popt, pcov = scipy.optimize.curve_fit(resolution_eq, x_ax, e_res, \n",
    "                                      bounds=([0., 0., 0.], [np.inf, np.inf, np.inf])) \n",
    "\n",
    "aa, bb, cc = popt\n",
    "\n",
    "\n",
    "# Calculate the residuals and chi-square\n",
    "residuals = e_res - resolution_eq(x_ax, *popt)\n",
    "data_err = e_res_err\n",
    "\n",
    "chi2 = np.sum(np.where(data_err > 0., (residuals/ data_err)**2, 0.))\n",
    "\n",
    "\n",
    "x_ax_fit = np.linspace(x_ax.min(), x_ax.max(), 100)\n",
    "best_fit_line = resolution_eq(x_ax_fit, aa, bb, cc)\n",
    "\n",
    "ax.plot(x_ax_fit, best_fit_line, '-', \n",
    "        label='Theoretical fit: a = %.2E, b = %.2f, c = %.2f; $\\chi^{2} = %.2f$'%(aa, bb, cc, chi2))\n",
    "\n",
    "    \n",
    "ax.legend(loc = 'best')\n",
    "\n",
    "ax.set_ylabel('Energy resolution [%]')\n",
    "ax.set_xlabel('Number of photoelectrons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c29e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
